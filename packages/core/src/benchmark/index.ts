/**
 * Incremark vs Traditional Parser Benchmark
 * 
 * å¯¹æ¯”å¢žé‡è§£æžå’Œä¼ ç»Ÿè§£æžï¼ˆæ¯æ¬¡é‡æ–°è§£æžå…¨éƒ¨å†…å®¹ï¼‰çš„æ€§èƒ½å·®å¼‚
 */

import { IncremarkParser } from '../parser/IncremarkParser'
import { fromMarkdown } from 'mdast-util-from-markdown'
import { gfm } from 'micromark-extension-gfm'
import { gfmFromMarkdown } from 'mdast-util-gfm'

// çŸ­æ–‡æœ¬æµ‹è¯•ï¼ˆ~800 å­—ç¬¦ï¼‰
const shortMarkdown = `
# Hello World

This is a paragraph with **bold** and *italic* text.

## Code Example

\`\`\`javascript
function hello() {
  console.log('Hello, World!');
  return {
    name: 'test',
    value: 42
  };
}
\`\`\`

## List Example

- Item 1
- Item 2
  - Nested item 2.1
  - Nested item 2.2
- Item 3

## Table Example

| Name | Age | City |
|------|-----|------|
| Alice | 25 | NYC |
| Bob | 30 | LA |

## Blockquote

> This is a quote
> with multiple lines
> and **formatted** text

## More Content

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.

### Subsection

More text here with [links](https://example.com) and \`inline code\`.

1. Ordered item 1
2. Ordered item 2
3. Ordered item 3

---

The end.
`

// ç”Ÿæˆé•¿æ–‡æœ¬ï¼ˆæ¨¡æ‹ŸçœŸå®ž AI è¾“å‡ºï¼‰
function generateLongMarkdown(targetLength: number): string {
  const sections = [
    `
# Introduction to Machine Learning

Machine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.

## Key Concepts

### Supervised Learning

In supervised learning, the algorithm learns from labeled training data, and makes predictions based on that data. Common algorithms include:

- **Linear Regression** - For predicting continuous values
- **Logistic Regression** - For classification problems
- **Decision Trees** - For both classification and regression
- **Random Forest** - Ensemble method using multiple decision trees
- **Support Vector Machines** - For classification with clear margins

\`\`\`python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train the model
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)
\`\`\`

### Unsupervised Learning

Unsupervised learning deals with unlabeled data. The algorithm tries to find patterns and relationships in the data.

| Algorithm | Use Case | Complexity |
|-----------|----------|------------|
| K-Means | Clustering | O(n*k*i) |
| DBSCAN | Density clustering | O(n log n) |
| PCA | Dimensionality reduction | O(n*dÂ²) |
| t-SNE | Visualization | O(nÂ²) |

> "The goal of unsupervised learning is to discover hidden patterns or data groupings without the need for human intervention." - Andrew Ng

`,
    `
## Deep Learning

Deep learning is a subset of machine learning that uses neural networks with many layers.

### Neural Network Architecture

\`\`\`
Input Layer â†’ Hidden Layer 1 â†’ Hidden Layer 2 â†’ ... â†’ Output Layer
     â†“              â†“                â†“                    â†“
  Features      Activations      Activations          Predictions
\`\`\`

### Common Activation Functions

1. **ReLU (Rectified Linear Unit)**
   - Formula: \`f(x) = max(0, x)\`
   - Most commonly used in hidden layers

2. **Sigmoid**
   - Formula: \`f(x) = 1 / (1 + e^(-x))\`
   - Used for binary classification

3. **Softmax**
   - Used for multi-class classification
   - Outputs probability distribution

\`\`\`python
import torch
import torch.nn as nn

class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(NeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        out = self.layer1(x)
        out = self.relu(out)
        out = self.layer2(out)
        return out
\`\`\`

`,
    `
## Natural Language Processing

NLP is a field of AI that focuses on the interaction between computers and humans through natural language.

### Key Tasks

- **Text Classification** - Categorizing text into predefined categories
- **Named Entity Recognition** - Identifying entities like names, locations, organizations
- **Sentiment Analysis** - Determining the emotional tone of text
- **Machine Translation** - Translating text from one language to another
- **Question Answering** - Answering questions based on context

### Transformer Architecture

The transformer architecture revolutionized NLP with the introduction of self-attention mechanisms.

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Transformer               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Encoder   â”‚  â”‚   Decoder   â”‚  â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚
â”‚  â”‚ Self-Attn   â”‚  â”‚ Self-Attn   â”‚  â”‚
â”‚  â”‚ Feed-Forwardâ”‚  â”‚ Cross-Attn  â”‚  â”‚
â”‚  â”‚             â”‚  â”‚ Feed-Forwardâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

> Transformers have become the foundation for large language models like GPT, BERT, and Claude.

`,
    `
## Best Practices

### Data Preprocessing

1. Handle missing values appropriately
2. Normalize or standardize numerical features
3. Encode categorical variables
4. Split data into train/validation/test sets
5. Apply data augmentation when appropriate

### Model Evaluation

| Metric | Formula | Use Case |
|--------|---------|----------|
| Accuracy | (TP+TN)/(TP+TN+FP+FN) | Balanced classes |
| Precision | TP/(TP+FP) | When FP is costly |
| Recall | TP/(TP+FN) | When FN is costly |
| F1 Score | 2*(P*R)/(P+R) | Imbalanced classes |
| AUC-ROC | Area under ROC curve | Binary classification |

### Hyperparameter Tuning

\`\`\`python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(
    estimator=RandomForestClassifier(),
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    verbose=2
)

grid_search.fit(X_train, y_train)
print(f"Best parameters: {grid_search.best_params_}")
\`\`\`

---

This concludes our overview of machine learning fundamentals.

`
  ]

  let result = ''
  let sectionIndex = 0
  
  while (result.length < targetLength) {
    result += sections[sectionIndex % sections.length]
    sectionIndex++
  }
  
  return result.slice(0, targetLength)
}

// é»˜è®¤æµ‹è¯•ç”¨çš„ Markdown å†…å®¹
const testMarkdown = shortMarkdown

interface BenchmarkResult {
  name: string
  totalTime: number
  parseCount: number
  avgTimePerParse: number
  totalCharsParsed: number
}

/**
 * æ¨¡æ‹Ÿæµå¼è¾“å…¥ï¼Œå°†æ–‡æœ¬æŒ‰ chunk å¤§å°åˆ†å‰²
 */
function simulateStream(text: string, chunkSize: number): string[] {
  const chunks: string[] = []
  for (let i = 0; i < text.length; i += chunkSize) {
    chunks.push(text.slice(i, i + chunkSize))
  }
  return chunks
}

/**
 * ä¼ ç»Ÿæ–¹å¼ï¼šæ¯æ¬¡æ”¶åˆ°æ–°å†…å®¹éƒ½é‡æ–°è§£æžå…¨éƒ¨æ–‡æœ¬
 */
function benchmarkTraditional(chunks: string[], iterations: number): BenchmarkResult {
  let totalTime = 0
  let totalCharsParsed = 0
  let parseCount = 0

  for (let iter = 0; iter < iterations; iter++) {
    let buffer = ''
    
    for (const chunk of chunks) {
      buffer += chunk
      
      const start = performance.now()
      fromMarkdown(buffer, {
        extensions: [gfm()],
        mdastExtensions: [gfmFromMarkdown()]
      })
      const end = performance.now()
      
      totalTime += (end - start)
      totalCharsParsed += buffer.length
      parseCount++
    }
  }

  return {
    name: 'Traditional (re-parse all)',
    totalTime,
    parseCount,
    avgTimePerParse: totalTime / parseCount,
    totalCharsParsed
  }
}

/**
 * Incremark æ–¹å¼ï¼šå¢žé‡è§£æž
 */
function benchmarkIncremental(chunks: string[], iterations: number): BenchmarkResult {
  let totalTime = 0
  let totalCharsParsed = 0
  let parseCount = 0

  for (let iter = 0; iter < iterations; iter++) {
    const parser = new IncremarkParser({ gfm: true })
    
    for (const chunk of chunks) {
      const start = performance.now()
      parser.append(chunk)
      const end = performance.now()
      
      totalTime += (end - start)
      totalCharsParsed += chunk.length
      parseCount++
    }
    
    const start = performance.now()
    parser.finalize()
    const end = performance.now()
    totalTime += (end - start)
  }

  return {
    name: 'Incremark (incremental)',
    totalTime,
    parseCount,
    avgTimePerParse: totalTime / parseCount,
    totalCharsParsed
  }
}

/**
 * è¿è¡Œ benchmark
 */
export function runBenchmark(options: {
  chunkSize?: number
  iterations?: number
  markdown?: string
  markdownLength?: number
} = {}) {
  const {
    chunkSize = 10,
    iterations = 100,
    markdownLength
  } = options
  
  // å¦‚æžœæŒ‡å®šäº†é•¿åº¦ï¼Œç”Ÿæˆå¯¹åº”é•¿åº¦çš„ Markdown
  const markdown = markdownLength 
    ? generateLongMarkdown(markdownLength) 
    : (options.markdown || testMarkdown)

  const chunks = simulateStream(markdown, chunkSize)
  
  console.log('='.repeat(60))
  console.log('Incremark Benchmark')
  console.log('='.repeat(60))
  console.log(`Markdown length: ${markdown.length} chars`)
  console.log(`Chunk size: ${chunkSize} chars`)
  console.log(`Total chunks: ${chunks.length}`)
  console.log(`Iterations: ${iterations}`)
  console.log('='.repeat(60))
  console.log('')

  // é¢„çƒ­
  console.log('Warming up...')
  benchmarkTraditional(chunks, 5)
  benchmarkIncremental(chunks, 5)
  console.log('')

  // æ­£å¼æµ‹è¯•
  console.log('Running benchmark...')
  console.log('')

  const traditional = benchmarkTraditional(chunks, iterations)
  const incremental = benchmarkIncremental(chunks, iterations)

  // è®¡ç®—èŠ‚çœç™¾åˆ†æ¯”
  const timeSaved = ((traditional.totalTime - incremental.totalTime) / traditional.totalTime * 100).toFixed(1)
  const charsSaved = ((traditional.totalCharsParsed - incremental.totalCharsParsed) / traditional.totalCharsParsed * 100).toFixed(1)

  console.log('Results:')
  console.log('-'.repeat(60))
  console.log('')
  
  console.log(`ðŸ“Š ${traditional.name}`)
  console.log(`   Total time: ${traditional.totalTime.toFixed(2)} ms`)
  console.log(`   Parse count: ${traditional.parseCount}`)
  console.log(`   Avg time per parse: ${traditional.avgTimePerParse.toFixed(4)} ms`)
  console.log(`   Total chars parsed: ${traditional.totalCharsParsed.toLocaleString()}`)
  console.log('')
  
  console.log(`âš¡ ${incremental.name}`)
  console.log(`   Total time: ${incremental.totalTime.toFixed(2)} ms`)
  console.log(`   Parse count: ${incremental.parseCount}`)
  console.log(`   Avg time per parse: ${incremental.avgTimePerParse.toFixed(4)} ms`)
  console.log(`   Total chars parsed: ${incremental.totalCharsParsed.toLocaleString()}`)
  console.log('')

  console.log('-'.repeat(60))
  console.log('')
  console.log(`ðŸŽ¯ Performance Improvement:`)
  console.log(`   Time saved: ${timeSaved}%`)
  console.log(`   Chars parsing saved: ${charsSaved}%`)
  console.log(`   Speedup: ${(traditional.totalTime / incremental.totalTime).toFixed(2)}x faster`)
  console.log('')
  console.log('='.repeat(60))

  return {
    traditional,
    incremental,
    timeSaved: parseFloat(timeSaved),
    charsSaved: parseFloat(charsSaved),
    speedup: traditional.totalTime / incremental.totalTime
  }
}

// å¦‚æžœç›´æŽ¥è¿è¡Œæ­¤æ–‡ä»¶
if (typeof process !== 'undefined' && process.argv[1]?.includes('benchmark')) {
  runBenchmark()
}

